Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
Training/evaluation parameters Namespace(adam_beta1=0.9, adam_beta2=0.98, adam_epsilon=1e-08, cache_dir='pretrained_model', config_name='', data_dir='dataset/conll03_distant/', device=device(type='cpu'), do_eval=True, do_lower_case=False, do_predict=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=1e-05, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', mt=0, mt_alpha1=0.99, mt_alpha2=0.995, mt_avg='exponential', mt_beta=10, mt_class='kl', mt_lambda=1, mt_loss_type='logits', mt_rampup=300, mt_updatefreq=1, n_gpu=0, no_cuda=False, num_train_epochs=50.0, output_dir='outputs/conll03/self_training/roberta_reinit0_begin900_period450_soft_hp5.9_50_1e-5/', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_train_batch_size=16, save_steps=100000, seed=0, self_training_begin_step=900, self_training_ensemble_label=0, self_training_hp_label=5.9, self_training_label_mode='soft', self_training_period=450, self_training_reinit=0, server_ip='', server_port='', tokenizer_name='', vat=0, vat_beta=1, vat_eps=0.001, vat_lambda=1, vat_loss_type='logits', warmup_steps=200, weight_decay=0.0001)
Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
Training/evaluation parameters Namespace(adam_beta1=0.9, adam_beta2=0.98, adam_epsilon=1e-08, cache_dir='pretrained_model', config_name='', data_dir='dataset/conll03_distant/', device=device(type='cpu'), do_eval=True, do_lower_case=False, do_predict=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=True, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=1e-05, local_rank=-1, logging_steps=100, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='roberta-base', model_type='roberta', mt=0, mt_alpha1=0.99, mt_alpha2=0.995, mt_avg='exponential', mt_beta=10, mt_class='kl', mt_lambda=1, mt_loss_type='logits', mt_rampup=300, mt_updatefreq=1, n_gpu=0, no_cuda=False, num_train_epochs=50.0, output_dir='outputs/conll03/self_training/roberta_reinit0_begin900_period450_soft_hp5.9_50_1e-5/', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_train_batch_size=16, save_steps=100000, seed=0, self_training_begin_step=900, self_training_ensemble_label=0, self_training_hp_label=5.9, self_training_label_mode='soft', self_training_period=450, self_training_reinit=0, server_ip='', server_port='', tokenizer_name='', vat=0, vat_beta=1, vat_eps=0.001, vat_lambda=1, vat_loss_type='logits', warmup_steps=200, weight_decay=0.0001)
***** Running training *****
  Num examples = 14041
  Num Epochs = 50
  Instantaneous batch size per GPU = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 43900
